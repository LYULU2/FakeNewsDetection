{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "improved_efficiency_baseline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1zlIHpYg5m3"
      },
      "source": [
        "**Note:** the previous baseline model is using countvecterizor to encode the text data, which result in an embedding with dimension of the the size of the whole vocabulary. With the large size of corpus and number of unique words, the model is very hard to train, thus word2vec is used instead in the baseline model to give a more realisic result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URQ3PcL4fxJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e3b886-ca3f-4d2b-ea88-d82aaef55454"
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seFlid44gQP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "138a4d42-1c3e-4e38-c01d-4100ef6571dd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAJ4W-9UgTnB"
      },
      "source": [
        "### import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8uGrYBggTEp"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn import feature_extraction,feature_selection\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout\n",
        "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.callbacks import History\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1canCHIxBkw"
      },
      "source": [
        "### read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKT_iX7dxBRx"
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/data/dataset.csv', delimiter = ',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU1TrEV5tXjF"
      },
      "source": [
        "vectorizer_count = feature_extraction.text.CountVectorizer()\n",
        "vectorizer_tfidf = feature_extraction.text.TfidfVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyilHHHDwJAE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7340cf91-a630-4ebb-b6b4-b6604f693572"
      },
      "source": [
        "vectorizer_count.fit(df['cleaned_text'])\n",
        "X_train_c = vectorizer_count.transform(df['cleaned_text'])\n",
        "dic_vocabulary_c = vectorizer_count.vocabulary_\n",
        "len(dic_vocabulary_c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "104456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNSpDghh07Lv"
      },
      "source": [
        "sns.heatmap(X_train_c.todense()[:,np.random.randint(0,X.shape[1],100)]==0, vmin=0, vmax=1, cbar=False).set_title('Sparse Matrix Sample')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYbmFyt8txzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce40e9d-71f4-4eb5-d6b3-d2fde70890fe"
      },
      "source": [
        "vectorizer_tfidf.fit(df['cleaned_text'])\n",
        "X_train_t = vectorizer_tfidf.transform(df['cleaned_text'])\n",
        "dic_vocabulary_t = vectorizer_tfidf.vocabulary_\n",
        "len(dic_vocabulary_t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "104456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2JEOrM508hU"
      },
      "source": [
        "sns.heatmap(X_train_c.todense()[:,np.random.randint(0,X.shape[1],100)]==0, vmin=0, vmax=1, cbar=False).set_title('Sparse Matrix Sample')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-BppZR0uJy0"
      },
      "source": [
        "### perform feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfKvMwcwuWXe"
      },
      "source": [
        "use chi-square test to select the realted fearures of label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26IiRMWquOI2"
      },
      "source": [
        "## no need to run this cell\n",
        "def optimize_vectorizer(vectorizer,X_train):\n",
        "  y = df[\"label\"]\n",
        "  X_names = vectorizer.get_feature_names()\n",
        "  p_value_limit = 0.95\n",
        "  dtf_features = pd.DataFrame()\n",
        "\n",
        "  for cat in np.unique(y):\n",
        "      chi2, p = feature_selection.chi2(X_train, y==cat)\n",
        "      dtf_features = dtf_features.append(pd.DataFrame(\n",
        "                    {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n",
        "      dtf_features = dtf_features.sort_values([\"y\",\"score\"], \n",
        "                      ascending=[True,False])\n",
        "      dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
        "  X_names = dtf_features[\"feature\"].unique().tolist()\n",
        "  return X_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFEBh3Y7uwrf"
      },
      "source": [
        "y = df[\"label\"]\n",
        "v_c = vectorizer_count.get_feature_names()\n",
        "p_value_limit = 0.95\n",
        "dtf_features = pd.DataFrame()\n",
        "for cat in np.unique(y):\n",
        "  chi2, p = feature_selection.chi2(X_train_c, y==cat)\n",
        "  dtf_features = dtf_features.append(pd.DataFrame(\n",
        "                    {\"feature\":v_c, \"score\":1-p, \"y\":cat}))\n",
        "  dtf_features = dtf_features.sort_values([\"y\",\"score\"], \n",
        "                      ascending=[True,False])\n",
        "  dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
        "v_c = dtf_features[\"feature\"].unique().tolist()\n",
        "\n",
        "vectorizer_count = feature_extraction.text.CountVectorizer(vocabulary=v_c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh3tr9ve-6n3"
      },
      "source": [
        "v_t = vectorizer_tfidf.get_feature_names()\n",
        "dtf_features = pd.DataFrame()\n",
        "for cat in np.unique(y):\n",
        "  chi2, p = feature_selection.chi2(X_train_t, y==cat)\n",
        "  dtf_features = dtf_features.append(pd.DataFrame(\n",
        "                    {\"feature\":v_t, \"score\":1-p, \"y\":cat}))\n",
        "  dtf_features = dtf_features.sort_values([\"y\",\"score\"], \n",
        "                      ascending=[True,False])\n",
        "  dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
        "v_t = dtf_features[\"feature\"].unique().tolist()\n",
        "\n",
        "vectorizer_tfidf = feature_extraction.text.TfidfVectorizer(vocabulary=v_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo-6ddjc0hUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e47915c-4728-4f9c-e4d4-447640743132"
      },
      "source": [
        "vectorizer_count.fit(df['cleaned_text'])\n",
        "X_train_c = vectorizer_count.transform(df['cleaned_text'])\n",
        "dic_vocabulary_c = vectorizer_count.vocabulary_\n",
        "len(dic_vocabulary_c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24717"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgO7ru4B03dM"
      },
      "source": [
        "sns.heatmap(X_train_c.todense()[:,np.random.randint(0,X.shape[1],100)]==0, vmin=0, vmax=1, cbar=False).set_title('Sparse Matrix Sample for count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qY6VMKA0iGa"
      },
      "source": [
        "vectorizer_tfidf.fit(df['cleaned_text'])\n",
        "X_train_t = vectorizer_tfidf.transform(df['cleaned_text'])\n",
        "dic_vocabulary_t = vectorizer_tfidf.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZR0s1FQ049n"
      },
      "source": [
        "sns.heatmap(X_train_c.todense()[:,np.random.randint(0,X.shape[1],100)]==0, vmin=0, vmax=1, cbar=False).set_title('Sparse Matrix Sample for tfidf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VSs1ekk3cEA"
      },
      "source": [
        "### embedding & classifier selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ07B3IbEYoX"
      },
      "source": [
        "def get_prediction(vectorizer, classifier, X_train, X_test, y_train, y_test):\n",
        "    pipe = Pipeline([('vector', vectorizer),\n",
        "                    ('model', classifier)])\n",
        "    model = pipe.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"Accuarcy: {}\".format(round(accuracy_score(y_test, y_pred)*100,2)))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"Confusion Matrix: \\n\", cm)\n",
        "    print(\"Classification Report: \\n\", classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw-guOemu8Us",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19af5c77-93f8-4602-e77e-deb2b7353b3d"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['label'], test_size = 0.3, random_state= 42)\n",
        "classifiers = [LogisticRegression(), SGDClassifier(), MultinomialNB(), BernoulliNB(), LinearSVC(),\n",
        "              KNeighborsClassifier(n_neighbors=5), DecisionTreeClassifier(), GradientBoostingClassifier(), \n",
        "               RandomForestClassifier(), XGBClassifier()]\n",
        "for classifier in classifiers:\n",
        "    print(\"\\n\\n\", classifier)\n",
        "    print(\"***********Usng Count Vectorizer****************\")\n",
        "    get_prediction(vectorizer_count, classifier, X_train, X_test, y_train, y_test)\n",
        "    print(\"***********Usng TFIDF Vectorizer****************\")\n",
        "    get_prediction(vectorizer_tfidf, classifier, X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "***********Usng Count Vectorizer****************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuarcy: 97.39\n",
            "Confusion Matrix: \n",
            " [[7829  173]\n",
            " [ 228 7140]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.98      8002\n",
            "           1       0.98      0.97      0.97      7368\n",
            "\n",
            "    accuracy                           0.97     15370\n",
            "   macro avg       0.97      0.97      0.97     15370\n",
            "weighted avg       0.97      0.97      0.97     15370\n",
            "\n",
            "***********Usng TFIDF Vectorizer****************\n",
            "Accuarcy: 96.3\n",
            "Confusion Matrix: \n",
            " [[7736  266]\n",
            " [ 303 7065]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.96      8002\n",
            "           1       0.96      0.96      0.96      7368\n",
            "\n",
            "    accuracy                           0.96     15370\n",
            "   macro avg       0.96      0.96      0.96     15370\n",
            "weighted avg       0.96      0.96      0.96     15370\n",
            "\n",
            "\n",
            "\n",
            " SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
            "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
            "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
            "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
            "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
            "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "***********Usng Count Vectorizer****************\n",
            "Accuarcy: 96.79\n",
            "Confusion Matrix: \n",
            " [[7835  167]\n",
            " [ 327 7041]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97      8002\n",
            "           1       0.98      0.96      0.97      7368\n",
            "\n",
            "    accuracy                           0.97     15370\n",
            "   macro avg       0.97      0.97      0.97     15370\n",
            "weighted avg       0.97      0.97      0.97     15370\n",
            "\n",
            "***********Usng TFIDF Vectorizer****************\n",
            "Accuarcy: 96.38\n",
            "Confusion Matrix: \n",
            " [[7803  199]\n",
            " [ 358 7010]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97      8002\n",
            "           1       0.97      0.95      0.96      7368\n",
            "\n",
            "    accuracy                           0.96     15370\n",
            "   macro avg       0.96      0.96      0.96     15370\n",
            "weighted avg       0.96      0.96      0.96     15370\n",
            "\n",
            "\n",
            "\n",
            " MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
            "***********Usng Count Vectorizer****************\n",
            "Accuarcy: 92.29\n",
            "Confusion Matrix: \n",
            " [[7429  573]\n",
            " [ 612 6756]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.93      8002\n",
            "           1       0.92      0.92      0.92      7368\n",
            "\n",
            "    accuracy                           0.92     15370\n",
            "   macro avg       0.92      0.92      0.92     15370\n",
            "weighted avg       0.92      0.92      0.92     15370\n",
            "\n",
            "***********Usng TFIDF Vectorizer****************\n",
            "Accuarcy: 90.77\n",
            "Confusion Matrix: \n",
            " [[7377  625]\n",
            " [ 794 6574]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.92      0.91      8002\n",
            "           1       0.91      0.89      0.90      7368\n",
            "\n",
            "    accuracy                           0.91     15370\n",
            "   macro avg       0.91      0.91      0.91     15370\n",
            "weighted avg       0.91      0.91      0.91     15370\n",
            "\n",
            "\n",
            "\n",
            " BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
            "***********Usng Count Vectorizer****************\n",
            "Accuarcy: 92.93\n",
            "Confusion Matrix: \n",
            " [[7496  506]\n",
            " [ 580 6788]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.94      0.93      8002\n",
            "           1       0.93      0.92      0.93      7368\n",
            "\n",
            "    accuracy                           0.93     15370\n",
            "   macro avg       0.93      0.93      0.93     15370\n",
            "weighted avg       0.93      0.93      0.93     15370\n",
            "\n",
            "***********Usng TFIDF Vectorizer****************\n",
            "Accuarcy: 92.34\n",
            "Confusion Matrix: \n",
            " [[7467  535]\n",
            " [ 642 6726]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.93      8002\n",
            "           1       0.93      0.91      0.92      7368\n",
            "\n",
            "    accuracy                           0.92     15370\n",
            "   macro avg       0.92      0.92      0.92     15370\n",
            "weighted avg       0.92      0.92      0.92     15370\n",
            "\n",
            "\n",
            "\n",
            " LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
            "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
            "          verbose=0)\n",
            "***********Usng Count Vectorizer****************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuarcy: 96.75\n",
            "Confusion Matrix: \n",
            " [[7785  217]\n",
            " [ 282 7086]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97      8002\n",
            "           1       0.97      0.96      0.97      7368\n",
            "\n",
            "    accuracy                           0.97     15370\n",
            "   macro avg       0.97      0.97      0.97     15370\n",
            "weighted avg       0.97      0.97      0.97     15370\n",
            "\n",
            "***********Usng TFIDF Vectorizer****************\n",
            "Accuarcy: 97.16\n",
            "Confusion Matrix: \n",
            " [[7810  192]\n",
            " [ 245 7123]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.97      8002\n",
            "           1       0.97      0.97      0.97      7368\n",
            "\n",
            "    accuracy                           0.97     15370\n",
            "   macro avg       0.97      0.97      0.97     15370\n",
            "weighted avg       0.97      0.97      0.97     15370\n",
            "\n",
            "\n",
            "\n",
            " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
            "                     weights='uniform')\n",
            "***********Usng Count Vectorizer****************\n",
            "Accuarcy: 79.67\n",
            "Confusion Matrix: \n",
            " [[6451 1551]\n",
            " [1573 5795]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.81      0.81      8002\n",
            "           1       0.79      0.79      0.79      7368\n",
            "\n",
            "    accuracy                           0.80     15370\n",
            "   macro avg       0.80      0.80      0.80     15370\n",
            "weighted avg       0.80      0.80      0.80     15370\n",
            "\n",
            "***********Usng TFIDF Vectorizer****************\n",
            "Accuarcy: 85.44\n",
            "Confusion Matrix: \n",
            " [[6289 1713]\n",
            " [ 525 6843]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.79      0.85      8002\n",
            "           1       0.80      0.93      0.86      7368\n",
            "\n",
            "    accuracy                           0.85     15370\n",
            "   macro avg       0.86      0.86      0.85     15370\n",
            "weighted avg       0.86      0.85      0.85     15370\n",
            "\n",
            "\n",
            "\n",
            " DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=None, splitter='best')\n",
            "***********Usng Count Vectorizer****************\n",
            "Accuarcy: 95.76\n",
            "Confusion Matrix: \n",
            " [[7722  280]\n",
            " [ 371 6997]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96      8002\n",
            "           1       0.96      0.95      0.96      7368\n",
            "\n",
            "    accuracy                           0.96     15370\n",
            "   macro avg       0.96      0.96      0.96     15370\n",
            "weighted avg       0.96      0.96      0.96     15370\n",
            "\n",
            "***********Usng TFIDF Vectorizer****************\n",
            "Accuarcy: 95.25\n",
            "Confusion Matrix: \n",
            " [[7695  307]\n",
            " [ 423 6945]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.96      0.95      8002\n",
            "           1       0.96      0.94      0.95      7368\n",
            "\n",
            "    accuracy                           0.95     15370\n",
            "   macro avg       0.95      0.95      0.95     15370\n",
            "weighted avg       0.95      0.95      0.95     15370\n",
            "\n",
            "\n",
            "\n",
            " GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
            "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
            "                           max_features=None, max_leaf_nodes=None,\n",
            "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                           min_samples_leaf=1, min_samples_split=2,\n",
            "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                           n_iter_no_change=None, presort='deprecated',\n",
            "                           random_state=None, subsample=1.0, tol=0.0001,\n",
            "                           validation_fraction=0.1, verbose=0,\n",
            "                           warm_start=False)\n",
            "***********Usng Count Vectorizer****************\n",
            "Accuarcy: 95.99\n",
            "Confusion Matrix: \n",
            " [[7859  143]\n",
            " [ 473 6895]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.98      0.96      8002\n",
            "           1       0.98      0.94      0.96      7368\n",
            "\n",
            "    accuracy                           0.96     15370\n",
            "   macro avg       0.96      0.96      0.96     15370\n",
            "weighted avg       0.96      0.96      0.96     15370\n",
            "\n",
            "***********Usng TFIDF Vectorizer****************\n",
            "Accuarcy: 96.07\n",
            "Confusion Matrix: \n",
            " [[7869  133]\n",
            " [ 471 6897]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.98      0.96      8002\n",
            "           1       0.98      0.94      0.96      7368\n",
            "\n",
            "    accuracy                           0.96     15370\n",
            "   macro avg       0.96      0.96      0.96     15370\n",
            "weighted avg       0.96      0.96      0.96     15370\n",
            "\n",
            "\n",
            "\n",
            " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False)\n",
            "***********Usng Count Vectorizer****************\n",
            "Accuarcy: 96.32\n",
            "Confusion Matrix: \n",
            " [[7834  168]\n",
            " [ 398 6970]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.98      0.97      8002\n",
            "           1       0.98      0.95      0.96      7368\n",
            "\n",
            "    accuracy                           0.96     15370\n",
            "   macro avg       0.96      0.96      0.96     15370\n",
            "weighted avg       0.96      0.96      0.96     15370\n",
            "\n",
            "***********Usng TFIDF Vectorizer****************\n",
            "Accuarcy: 96.49\n",
            "Confusion Matrix: \n",
            " [[7900  102]\n",
            " [ 438 6930]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.99      0.97      8002\n",
            "           1       0.99      0.94      0.96      7368\n",
            "\n",
            "    accuracy                           0.96     15370\n",
            "   macro avg       0.97      0.96      0.96     15370\n",
            "weighted avg       0.97      0.96      0.96     15370\n",
            "\n",
            "\n",
            "\n",
            " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
            "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
            "              nthread=None, objective='binary:logistic', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "              silent=None, subsample=1, verbosity=1)\n",
            "***********Usng Count Vectorizer****************\n",
            "Accuarcy: 96.18\n",
            "Confusion Matrix: \n",
            " [[7861  141]\n",
            " [ 446 6922]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.98      0.96      8002\n",
            "           1       0.98      0.94      0.96      7368\n",
            "\n",
            "    accuracy                           0.96     15370\n",
            "   macro avg       0.96      0.96      0.96     15370\n",
            "weighted avg       0.96      0.96      0.96     15370\n",
            "\n",
            "***********Usng TFIDF Vectorizer****************\n",
            "Accuarcy: 96.1\n",
            "Confusion Matrix: \n",
            " [[7857  145]\n",
            " [ 454 6914]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.98      0.96      8002\n",
            "           1       0.98      0.94      0.96      7368\n",
            "\n",
            "    accuracy                           0.96     15370\n",
            "   macro avg       0.96      0.96      0.96     15370\n",
            "weighted avg       0.96      0.96      0.96     15370\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgAiXJRn3ktf"
      },
      "source": [
        "### parameter selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdt0DNVU3mbA"
      },
      "source": [
        "from pprint import pprint\n",
        "from time import time\n",
        "import logging\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5sCUd843s5B"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    #('tfidf', TfidfVectorizer()),\n",
        "    ('clf', LogisticRegression()),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC3dinYG3yRU"
      },
      "source": [
        "parameters = {\n",
        "    'vect__max_df': (0.5, 0.75, 1.0),\n",
        "    'vect__max_features': (None, 5000, 10000, 50000),\n",
        "    'vect__ngram_range': ((1, 1), (2, 2), (1, 2)),\n",
        "    #'tfidf__use_idf': (True, False),\n",
        "    #'tfidf__norm': ('l1', 'l2'),\n",
        "    'clf__penalty': ('l1', 'l2'),\n",
        "    'clf__max_iter': (50,100,200),\n",
        "    #'clf__n_neighbors': list(range(2,8,2)),\n",
        "    #'clf__p': (1,2),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv4-Wqex3z61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40531806-ee4b-4777-fdb8-e99debb3826a"
      },
      "source": [
        "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
        "\n",
        "print(\"Performing grid search...\")\n",
        "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
        "print(\"parameters:\")\n",
        "pprint(parameters)\n",
        "t0 = time()\n",
        "grid_search.fit(df['cleaned_text'], df['label'])\n",
        "print(\"done in %0.3fs\" % (time() - t0))\n",
        "print()\n",
        "\n",
        "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters = grid_search.best_estimator_.get_params()\n",
        "for param_name in sorted(parameters.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing grid search...\n",
            "pipeline: ['vect', 'clf']\n",
            "parameters:\n",
            "{'clf__max_iter': (50, 100, 200),\n",
            " 'clf__penalty': ('l1', 'l2'),\n",
            " 'vect__max_df': (0.5, 0.75, 1.0),\n",
            " 'vect__max_features': (None, 5000, 10000, 50000),\n",
            " 'vect__ngram_range': ((1, 1), (2, 2), (1, 2))}\n",
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 14.1min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 62.0min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed: 151.2min\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed: 283.4min\n",
            "[Parallel(n_jobs=-1)]: Done 1080 out of 1080 | elapsed: 411.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done in 24910.327s\n",
            "\n",
            "Best score: 0.935\n",
            "Best parameters set:\n",
            "\tclf__max_iter: 200\n",
            "\tclf__penalty: 'l2'\n",
            "\tvect__max_df: 0.75\n",
            "\tvect__max_features: None\n",
            "\tvect__ngram_range: (1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzcMol5EIgHY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}